{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kerasに慣れよう1\n",
    "入力ベクトル$(x_{1},x_{2},x_{3},x_{4},x_{5})$ ($x_i \\in [0,1]$)の各要素の加算結果が2.5以上で1,未満で0を出すモデル  \n",
    "$f(x) = if\\ \\sum^5_{i=1}x_i \\ \\geq\\ 2.5 \\ then\\  1\\  else\\  0$  \n",
    "参考:[無から始めるKeras 第1回][1]\n",
    "[1]:https://qiita.com/Ishotihadus/items/c2f864c0cde3d17b7efb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリなどの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#ライブラリ\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "#GPUの仕様に関する設定\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options = tf.GPUOptions(\n",
    "        visible_device_list = \"0\",\n",
    "        allow_growth = True,\n",
    "        per_process_gpu_memory_fraction = 0.3))\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ\n",
      "[[0.63824372 0.51774281 0.11788814 0.8232685  0.94268798]\n",
      " [0.2111682  0.51023754 0.48468407 0.60537811 0.15774433]\n",
      " [0.13077407 0.65198469 0.33986234 0.69184605 0.57739458]]\n",
      "正解(onehot)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#学習データセット\n",
    "data = np.random.rand(10000,5) #1000個の5次元ベクトル\n",
    "labels = (np.sum(data, axis=1) > 2.5) * 1 #ラベル(0,1)\n",
    "labels = np_utils.to_categorical(labels)  #ラベル(onehot)\n",
    "#onehotラベルはこんな感じ\n",
    "#学習データ\n",
    "print(\"学習データ\")\n",
    "print(data[:3])\n",
    "print(\"正解(onehot)\")\n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20)                120       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 162\n",
      "Trainable params: 162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Sequential:層を積み上げる単純なモデル\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#こんな書き方もできる\n",
    "# model = Sequential([\n",
    "#     Dense(20, input_dim=5),\n",
    "#     Activation('relu'),\n",
    "#     Dense(2,activation='softmax')\n",
    "# ])\n",
    "\n",
    "#ネットワーク構造の出力\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.6726 - acc: 0.5425 - val_loss: 0.6510 - val_acc: 0.6005\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6219 - acc: 0.6561 - val_loss: 0.5908 - val_acc: 0.7110\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5509 - acc: 0.7800 - val_loss: 0.5123 - val_acc: 0.8300\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4668 - acc: 0.8726 - val_loss: 0.4295 - val_acc: 0.8965\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3872 - acc: 0.9237 - val_loss: 0.3578 - val_acc: 0.9465\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.3224 - acc: 0.9616 - val_loss: 0.3024 - val_acc: 0.9625\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.2740 - acc: 0.9750 - val_loss: 0.2617 - val_acc: 0.9820\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.2389 - acc: 0.9800 - val_loss: 0.2320 - val_acc: 0.9900\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.2126 - acc: 0.9854 - val_loss: 0.2086 - val_acc: 0.9915\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1917 - acc: 0.9896 - val_loss: 0.1905 - val_acc: 0.9920\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1754 - acc: 0.9920 - val_loss: 0.1760 - val_acc: 0.9935\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.1622 - acc: 0.9904 - val_loss: 0.1634 - val_acc: 0.9915\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1512 - acc: 0.9919 - val_loss: 0.1531 - val_acc: 0.9870\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1420 - acc: 0.9929 - val_loss: 0.1447 - val_acc: 0.9935\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1342 - acc: 0.9904 - val_loss: 0.1375 - val_acc: 0.9810\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.1271 - acc: 0.9911 - val_loss: 0.1306 - val_acc: 0.9930\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1214 - acc: 0.9893 - val_loss: 0.1257 - val_acc: 0.9915\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1162 - acc: 0.9908 - val_loss: 0.1202 - val_acc: 0.9830\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.1113 - acc: 0.9903 - val_loss: 0.1152 - val_acc: 0.9950\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1067 - acc: 0.9945 - val_loss: 0.1118 - val_acc: 0.9920\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1029 - acc: 0.9933 - val_loss: 0.1074 - val_acc: 0.9905\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0992 - acc: 0.9933 - val_loss: 0.1037 - val_acc: 0.9940\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0959 - acc: 0.9940 - val_loss: 0.1011 - val_acc: 0.9950\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0931 - acc: 0.9924 - val_loss: 0.0982 - val_acc: 0.9830\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0904 - acc: 0.9939 - val_loss: 0.0950 - val_acc: 0.9945\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0875 - acc: 0.9931 - val_loss: 0.0926 - val_acc: 0.9880\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0854 - acc: 0.9928 - val_loss: 0.0902 - val_acc: 0.9940\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0829 - acc: 0.9933 - val_loss: 0.0883 - val_acc: 0.9940\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0812 - acc: 0.9939 - val_loss: 0.0852 - val_acc: 0.9950\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0791 - acc: 0.9938 - val_loss: 0.0846 - val_acc: 0.9915\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0772 - acc: 0.9939 - val_loss: 0.0817 - val_acc: 0.9945\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0755 - acc: 0.9931 - val_loss: 0.0805 - val_acc: 0.9940\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0738 - acc: 0.9945 - val_loss: 0.0787 - val_acc: 0.9940\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0720 - acc: 0.9949 - val_loss: 0.0768 - val_acc: 0.9950\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0706 - acc: 0.9950 - val_loss: 0.0749 - val_acc: 0.9955\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0692 - acc: 0.9955 - val_loss: 0.0740 - val_acc: 0.9950\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0679 - acc: 0.9954 - val_loss: 0.0724 - val_acc: 0.9945\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0673 - acc: 0.9928 - val_loss: 0.0715 - val_acc: 0.9950\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0653 - acc: 0.9949 - val_loss: 0.0714 - val_acc: 0.9910\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0647 - acc: 0.9934 - val_loss: 0.0687 - val_acc: 0.9950\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0633 - acc: 0.9936 - val_loss: 0.0676 - val_acc: 0.9945\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0619 - acc: 0.9949 - val_loss: 0.0673 - val_acc: 0.9875\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0613 - acc: 0.9941 - val_loss: 0.0655 - val_acc: 0.9940\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0605 - acc: 0.9920 - val_loss: 0.0643 - val_acc: 0.9960\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0591 - acc: 0.9946 - val_loss: 0.0635 - val_acc: 0.9950\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0582 - acc: 0.9951 - val_loss: 0.0640 - val_acc: 0.9915\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0576 - acc: 0.9945 - val_loss: 0.0614 - val_acc: 0.9960\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0566 - acc: 0.9970 - val_loss: 0.0607 - val_acc: 0.9950\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0563 - acc: 0.9940 - val_loss: 0.0601 - val_acc: 0.9950\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0552 - acc: 0.9955 - val_loss: 0.0592 - val_acc: 0.9945\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0542 - acc: 0.9960 - val_loss: 0.0581 - val_acc: 0.9960\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0534 - acc: 0.9954 - val_loss: 0.0577 - val_acc: 0.9950\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0532 - acc: 0.9950 - val_loss: 0.0567 - val_acc: 0.9970\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0524 - acc: 0.9945 - val_loss: 0.0562 - val_acc: 0.9950\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0517 - acc: 0.9941 - val_loss: 0.0561 - val_acc: 0.9920\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0513 - acc: 0.9950 - val_loss: 0.0548 - val_acc: 0.9955\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0505 - acc: 0.9948 - val_loss: 0.0542 - val_acc: 0.9955\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.0507 - acc: 0.9924 - val_loss: 0.0536 - val_acc: 0.9975\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.0494 - acc: 0.9944 - val_loss: 0.0530 - val_acc: 0.9945\n",
      "Epoch 60/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0490 - acc: 0.9934 - val_loss: 0.0525 - val_acc: 0.9950\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0483 - acc: 0.9950 - val_loss: 0.0518 - val_acc: 0.9960\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0475 - acc: 0.9960 - val_loss: 0.0525 - val_acc: 0.9915\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.0472 - acc: 0.9953 - val_loss: 0.0511 - val_acc: 0.9950\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0469 - acc: 0.9953 - val_loss: 0.0503 - val_acc: 0.9955\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0462 - acc: 0.9963 - val_loss: 0.0500 - val_acc: 0.9955\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0459 - acc: 0.9945 - val_loss: 0.0493 - val_acc: 0.9950\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0456 - acc: 0.9936 - val_loss: 0.0491 - val_acc: 0.9950\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0447 - acc: 0.9964 - val_loss: 0.0485 - val_acc: 0.9950\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0443 - acc: 0.9960 - val_loss: 0.0479 - val_acc: 0.9945\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0442 - acc: 0.9946 - val_loss: 0.0476 - val_acc: 0.9955\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0435 - acc: 0.9951 - val_loss: 0.0470 - val_acc: 0.9950\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0432 - acc: 0.9966 - val_loss: 0.0466 - val_acc: 0.9975\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0427 - acc: 0.9964 - val_loss: 0.0464 - val_acc: 0.9950\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0427 - acc: 0.9959 - val_loss: 0.0461 - val_acc: 0.9950\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0422 - acc: 0.9954 - val_loss: 0.0459 - val_acc: 0.9950\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0417 - acc: 0.9960 - val_loss: 0.0455 - val_acc: 0.9955\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0414 - acc: 0.9954 - val_loss: 0.0453 - val_acc: 0.9925\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0411 - acc: 0.9961 - val_loss: 0.0448 - val_acc: 0.9950\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0410 - acc: 0.9954 - val_loss: 0.0437 - val_acc: 0.9960\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0405 - acc: 0.9954 - val_loss: 0.0435 - val_acc: 0.9960\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0402 - acc: 0.9961 - val_loss: 0.0431 - val_acc: 0.9955\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0398 - acc: 0.9945 - val_loss: 0.0431 - val_acc: 0.9955\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0394 - acc: 0.9949 - val_loss: 0.0426 - val_acc: 0.9970\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0395 - acc: 0.9949 - val_loss: 0.0445 - val_acc: 0.9910\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0387 - acc: 0.9961 - val_loss: 0.0419 - val_acc: 0.9960\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0388 - acc: 0.9949 - val_loss: 0.0431 - val_acc: 0.9915\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0383 - acc: 0.9961 - val_loss: 0.0412 - val_acc: 0.9955\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0380 - acc: 0.9954 - val_loss: 0.0422 - val_acc: 0.9925\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0377 - acc: 0.9958 - val_loss: 0.0405 - val_acc: 0.9960\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0373 - acc: 0.9954 - val_loss: 0.0404 - val_acc: 0.9970\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0375 - acc: 0.9949 - val_loss: 0.0402 - val_acc: 0.9950\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.0369 - acc: 0.9969 - val_loss: 0.0409 - val_acc: 0.9925\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0367 - acc: 0.9951 - val_loss: 0.0400 - val_acc: 0.9945\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0363 - acc: 0.9968 - val_loss: 0.0392 - val_acc: 0.9955\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0362 - acc: 0.9959 - val_loss: 0.0391 - val_acc: 0.9965\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0360 - acc: 0.9961 - val_loss: 0.0389 - val_acc: 0.9965\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0357 - acc: 0.9954 - val_loss: 0.0409 - val_acc: 0.9910\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0353 - acc: 0.9959 - val_loss: 0.0383 - val_acc: 0.9965\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0352 - acc: 0.9964 - val_loss: 0.0379 - val_acc: 0.9960\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.0351 - acc: 0.9959 - val_loss: 0.0379 - val_acc: 0.9965\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.0346 - acc: 0.9964 - val_loss: 0.0376 - val_acc: 0.9955\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0346 - acc: 0.9959 - val_loss: 0.0377 - val_acc: 0.9960\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0344 - acc: 0.9963 - val_loss: 0.0371 - val_acc: 0.9960\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0343 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9930\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0345 - acc: 0.9943 - val_loss: 0.0365 - val_acc: 0.9965\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0339 - acc: 0.9956 - val_loss: 0.0374 - val_acc: 0.9950\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0337 - acc: 0.9958 - val_loss: 0.0361 - val_acc: 0.9960\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0332 - acc: 0.9966 - val_loss: 0.0367 - val_acc: 0.9950\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0332 - acc: 0.9954 - val_loss: 0.0358 - val_acc: 0.9960\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0330 - acc: 0.9951 - val_loss: 0.0361 - val_acc: 0.9955\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0330 - acc: 0.9943 - val_loss: 0.0356 - val_acc: 0.9965\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0329 - acc: 0.9959 - val_loss: 0.0369 - val_acc: 0.9915\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0325 - acc: 0.9960 - val_loss: 0.0350 - val_acc: 0.9955\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0327 - acc: 0.9950 - val_loss: 0.0348 - val_acc: 0.9960\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.0322 - acc: 0.9958 - val_loss: 0.0351 - val_acc: 0.9965\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0322 - acc: 0.9951 - val_loss: 0.0350 - val_acc: 0.9960\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.0319 - acc: 0.9973 - val_loss: 0.0344 - val_acc: 0.9950\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0313 - acc: 0.9971 - val_loss: 0.0343 - val_acc: 0.9970\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0315 - acc: 0.9964 - val_loss: 0.0339 - val_acc: 0.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0314 - acc: 0.9961 - val_loss: 0.0341 - val_acc: 0.9960\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0312 - acc: 0.9959 - val_loss: 0.0337 - val_acc: 0.9955\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0342 - val_acc: 0.9945\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0308 - acc: 0.9959 - val_loss: 0.0334 - val_acc: 0.9955\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0307 - acc: 0.9965 - val_loss: 0.0332 - val_acc: 0.9955\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0305 - acc: 0.9963 - val_loss: 0.0330 - val_acc: 0.9950\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0304 - acc: 0.9963 - val_loss: 0.0330 - val_acc: 0.9965\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0302 - acc: 0.9971 - val_loss: 0.0330 - val_acc: 0.9955\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0300 - acc: 0.9965 - val_loss: 0.0332 - val_acc: 0.9955\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0304 - acc: 0.9949 - val_loss: 0.0339 - val_acc: 0.9935\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0302 - acc: 0.9961 - val_loss: 0.0328 - val_acc: 0.9935\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0298 - acc: 0.9973 - val_loss: 0.0332 - val_acc: 0.9940\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0297 - acc: 0.9973 - val_loss: 0.0319 - val_acc: 0.9960\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0295 - acc: 0.9963 - val_loss: 0.0320 - val_acc: 0.9965\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0299 - acc: 0.9949 - val_loss: 0.0316 - val_acc: 0.9965\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0296 - acc: 0.9956 - val_loss: 0.0317 - val_acc: 0.9950\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0292 - acc: 0.9964 - val_loss: 0.0315 - val_acc: 0.9960\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.0291 - acc: 0.9953 - val_loss: 0.0314 - val_acc: 0.9950\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.0293 - acc: 0.9949 - val_loss: 0.0345 - val_acc: 0.9895\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9951 - val_loss: 0.0312 - val_acc: 0.9965\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0288 - acc: 0.9956 - val_loss: 0.0312 - val_acc: 0.9965\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0283 - acc: 0.9964 - val_loss: 0.0319 - val_acc: 0.9920\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0287 - acc: 0.9964 - val_loss: 0.0310 - val_acc: 0.9950\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0284 - acc: 0.9965 - val_loss: 0.0329 - val_acc: 0.9920\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0287 - acc: 0.9961 - val_loss: 0.0304 - val_acc: 0.9950\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.0280 - acc: 0.9964 - val_loss: 0.0302 - val_acc: 0.9950\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.0283 - acc: 0.9950 - val_loss: 0.0301 - val_acc: 0.9965\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.0279 - acc: 0.9971 - val_loss: 0.0298 - val_acc: 0.9960\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0278 - acc: 0.9970 - val_loss: 0.0297 - val_acc: 0.9960\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.0274 - acc: 0.9963 - val_loss: 0.0301 - val_acc: 0.9955\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.0274 - acc: 0.9961 - val_loss: 0.0299 - val_acc: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6792133e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compli:最適化関数,損失関数,評価指標\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#学習\n",
    "#fit;入力，出力，バッチサイズ(一度の更新に使用するデータ数), エポック数(学習回数),検証セットの割合(ランダムではないので注意)\n",
    "model.fit(data, labels, batch_size = 100, epochs=150, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 未知データの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価データセット\n",
    "test_data = np.random.rand(1000,5)\n",
    "test_label = (np.sum(test_data, axis=1) > 2.5) * 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力\n",
      "[0.42922686 0.10192073 0.70005548 0.57467436 0.59286714]\n",
      "正解\n",
      "0\n",
      "予測結果の確率(モデルの出力)\n",
      "[[0.98598576 0.01401428]]\n",
      "確率が最大の次元\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"入力\")\n",
    "print(test_data[0])\n",
    "print(\"正解\")\n",
    "print(test_label[0])\n",
    "print(\"予測結果の確率(モデルの出力)\")\n",
    "#出力結果の確率\n",
    "print(model.predict(test_data[0:1]))\n",
    "print(\"確率が最大の次元\")\n",
    "#最も大きい次元を出力とする\n",
    "print(np.argmax(model.predict(test_data[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各データに対する予測\n",
      "[0 0 1]\n",
      "正解と予測の比較\n",
      "[ True  True  True]\n",
      "認識率\n",
      "0.996\n"
     ]
    }
   ],
   "source": [
    "predict = np.argmax(model.predict(test_data), axis=1)\n",
    "#predictの中身\n",
    "print(\"各データに対する予測\")\n",
    "print(predict[:3])\n",
    "print(\"正解と予測の比較\")\n",
    "print((predict==test_label)[:3])\n",
    "#sumを取るとtrueの数を集計する\n",
    "print(\"認識率\")\n",
    "print(sum(predict == test_label) /1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの中身(重みとバイアスを取得)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07245422 -0.76324856  0.7859789  -0.31263816  0.28501177 -0.52260983\n",
      "  -0.5815298   0.87844     0.8093793  -0.22135888 -0.2908083   0.7141679\n",
      "   0.73738694 -0.4658815  -0.36737716 -0.10367944 -0.38862735  0.35797924\n",
      "   0.6213824   0.86724794]\n",
      " [-0.5889794  -0.20837772  0.5896446  -0.44063967 -0.02502468 -0.12219965\n",
      "   0.0246519   0.70582134  1.0986743   0.05536262 -0.79370195  0.61077756\n",
      "   0.72866404  0.17121376 -0.4394324   0.06184224 -0.3990213   0.9721677\n",
      "   0.59114563  1.0119864 ]\n",
      " [-0.4692893  -0.3926169   0.78621554 -0.06523511 -0.6226341  -0.4768676\n",
      "  -0.9288128   0.5054797   0.3359503  -0.37780178 -0.24653074  0.89912605\n",
      "   0.86226207 -0.1142491   0.2296202  -0.58266085 -0.21870618  0.6524426\n",
      "   0.7657701   0.2795029 ]\n",
      " [-0.4653854  -0.436262    0.70475334 -0.11117026  0.35039404 -0.36173663\n",
      "   0.145712    0.6153372   0.85476017  0.0274701  -0.1293269   0.5208152\n",
      "   0.8943596  -0.1378539  -0.48768464 -0.69694525 -0.5728259   0.41966832\n",
      "   0.71036476  0.9184199 ]\n",
      " [-0.6655019  -0.26664314  0.96112096 -0.10961017  0.3158137  -0.3202137\n",
      "  -0.36753747  0.44779402  0.7363253  -0.03340299 -0.51322097  0.6856903\n",
      "   0.4510608  -0.3784607   0.07406468 -0.29019728 -0.5504501   0.64677966\n",
      "   0.90890175  0.48856306]]\n",
      "[ 1.5408659   1.4305803  -1.3417217   0.         -0.33660418  1.2433674\n",
      "  1.3500997  -1.2037374  -1.3835615  -0.05243485  1.4585983  -1.3481423\n",
      " -1.4031206  -0.04672286 -0.12037642  1.3602842   1.3832964  -1.119103\n",
      " -1.400681   -1.2137871 ]\n",
      "[[ 2.2712274  -2.8471391 ]\n",
      " [ 2.6660204  -2.7572129 ]\n",
      " [-1.8256518   1.8801469 ]\n",
      " [-0.24592227  0.43033522]\n",
      " [-1.4332932   0.95606834]\n",
      " [ 2.2467277  -2.268788  ]\n",
      " [ 1.8968028  -1.6855454 ]\n",
      " [-2.2649732   2.6548798 ]\n",
      " [-1.7503144   2.3130999 ]\n",
      " [-0.01721715  0.19066855]\n",
      " [ 2.1569316  -2.5983193 ]\n",
      " [-2.563395    2.75585   ]\n",
      " [-2.3601825   2.3814852 ]\n",
      " [-0.34116012  0.24714983]\n",
      " [-0.13633475  0.23939809]\n",
      " [ 2.2478082  -2.10608   ]\n",
      " [ 3.1326904  -3.1863623 ]\n",
      " [-2.253912    2.4594395 ]\n",
      " [-2.1117616   2.8418746 ]\n",
      " [-1.9960651   1.7528417 ]]\n",
      "[ 0.9132983 -0.9132982]\n"
     ]
    }
   ],
   "source": [
    "weight = model.get_weights()\n",
    "print(weight[0]) #隠れ層重み\n",
    "print(weight[1]) #隠れ層バイアス\n",
    "print(weight[2]) #出力層重み\n",
    "print(weight[3]) #出力層バイアス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
